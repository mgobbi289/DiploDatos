{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Manejo de Datos en PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "import gzip\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.parsing import preprocessing\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## La clase `Dataset`\n",
    "\n",
    "La clase abstracta [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) es la clase base para construir un dataset de *PyTorch*. Cualquier dataset personalizado debe heredar de dicha clase e implementar los siguientes métodos:\n",
    "\n",
    "- `__len__`: Para que `len(dataset)` devuelva el tamaño del conjunto de datos.\n",
    "- `__getitem__`: Para soportar indexado de manera que `dataset[i]` devuelva el elemento `i`. Es común que en ciertos casos se utilice este método para levantar el dato real (e.g. una imagen) mientras que lo que se guarde en el dataset sea sólo una referencia a dicho dato (e.g. un path a la imagen). De esta manera se evita cargar muchas imágenes en memoria, haciendo que sea menos demandante a nivel RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 50000 elements\n",
      "Sample element...\n",
      "{'data': \"One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\", 'target': 'positive'}\n"
     ]
    }
   ],
   "source": [
    "class IMDBReviewsDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.dataset = pd.read_csv(path)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if torch.is_tensor(item):\n",
    "            item = item.tolist() # Deal with list of items instead of tensor.\n",
    "\n",
    "        item = {\n",
    "            'data': self.dataset.iloc[item]['review'],\n",
    "            'target': self.dataset.iloc[item]['sentiment']\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "        \n",
    "        return item\n",
    "\n",
    "dataset = IMDBReviewsDataset('data/imdb_reviews.csv.gz')\n",
    "\n",
    "print(f'Dataset loaded with {len(dataset)} elements')\n",
    "print(f'Sample element...\\n{dataset[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transformaciones\n",
    "\n",
    "El ejemplo anterior nos muestra el uso básico, pero claramente no podemos pasarle eso a una red neuronal, ya que no puede manejar texto. Es para eso que tenemos que hacer algún tipo de transformación sobre los atributos (en este caso el único atributo es el texto)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalización\n",
    "\n",
    "En particular, como vemos en el caso anterior, el texto no está normalizado, donde parte de las transformaciones pueden incluir realizar algún tipo de normalización. Para eso hagamos uso de [`gensim`](https://radimrehurek.com/gensim/index.html), en particular utilizaremos el módulo [`preprocessing`](https://radimrehurek.com/gensim/parsing/preprocessing.html#module-gensim.parsing.preprocessing) que se encargará de hacer varias normalizaciones por defecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': ['reviewers',\n",
       "  'mentioned',\n",
       "  'watching',\n",
       "  'episode',\n",
       "  'hooked',\n",
       "  'right',\n",
       "  'exactly',\n",
       "  'happened',\n",
       "  'thing',\n",
       "  'struck',\n",
       "  'brutality',\n",
       "  'unflinching',\n",
       "  'scenes',\n",
       "  'violence',\n",
       "  'set',\n",
       "  'right',\n",
       "  'word',\n",
       "  'trust',\n",
       "  'faint',\n",
       "  'hearted',\n",
       "  'timid',\n",
       "  'pulls',\n",
       "  'punches',\n",
       "  'regards',\n",
       "  'drugs',\n",
       "  'sex',\n",
       "  'violence',\n",
       "  'hardcore',\n",
       "  'classic',\n",
       "  'use',\n",
       "  'word',\n",
       "  'called',\n",
       "  'nickname',\n",
       "  'given',\n",
       "  'oswald',\n",
       "  'maximum',\n",
       "  'security',\n",
       "  'state',\n",
       "  'penitentary',\n",
       "  'focuses',\n",
       "  'mainly',\n",
       "  'emerald',\n",
       "  'city',\n",
       "  'experimental',\n",
       "  'section',\n",
       "  'prison',\n",
       "  'cells',\n",
       "  'glass',\n",
       "  'fronts',\n",
       "  'face',\n",
       "  'inwards',\n",
       "  'privacy',\n",
       "  'high',\n",
       "  'agenda',\n",
       "  'city',\n",
       "  'home',\n",
       "  'aryans',\n",
       "  'muslims',\n",
       "  'gangstas',\n",
       "  'latinos',\n",
       "  'christians',\n",
       "  'italians',\n",
       "  'irish',\n",
       "  'scuffles',\n",
       "  'death',\n",
       "  'stares',\n",
       "  'dodgy',\n",
       "  'dealings',\n",
       "  'shady',\n",
       "  'agreements',\n",
       "  'far',\n",
       "  'away',\n",
       "  'main',\n",
       "  'appeal',\n",
       "  'fact',\n",
       "  'goes',\n",
       "  'shows',\n",
       "  'wouldn',\n",
       "  'dare',\n",
       "  'forget',\n",
       "  'pretty',\n",
       "  'pictures',\n",
       "  'painted',\n",
       "  'mainstream',\n",
       "  'audiences',\n",
       "  'forget',\n",
       "  'charm',\n",
       "  'forget',\n",
       "  'romance',\n",
       "  'mess',\n",
       "  'episode',\n",
       "  'saw',\n",
       "  'struck',\n",
       "  'nasty',\n",
       "  'surreal',\n",
       "  'couldn',\n",
       "  'ready',\n",
       "  'watched',\n",
       "  'developed',\n",
       "  'taste',\n",
       "  'got',\n",
       "  'accustomed',\n",
       "  'high',\n",
       "  'levels',\n",
       "  'graphic',\n",
       "  'violence',\n",
       "  'violence',\n",
       "  'injustice',\n",
       "  'crooked',\n",
       "  'guards',\n",
       "  'sold',\n",
       "  'nickel',\n",
       "  'inmates',\n",
       "  'kill',\n",
       "  'order',\n",
       "  'away',\n",
       "  'mannered',\n",
       "  'middle',\n",
       "  'class',\n",
       "  'inmates',\n",
       "  'turned',\n",
       "  'prison',\n",
       "  'bitches',\n",
       "  'lack',\n",
       "  'street',\n",
       "  'skills',\n",
       "  'prison',\n",
       "  'experience',\n",
       "  'watching',\n",
       "  'comfortable',\n",
       "  'uncomfortable',\n",
       "  'viewing',\n",
       "  'thats',\n",
       "  'touch',\n",
       "  'darker'],\n",
       " 'target': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextPreprocess:\n",
    "    def __init__(self, filters=None):\n",
    "        if filters:\n",
    "            self.filters = filters\n",
    "        else:\n",
    "            # Predefined filters...\n",
    "            self.filters = [\n",
    "                lambda s: s.lower(),\n",
    "                preprocessing.strip_tags,\n",
    "                preprocessing.strip_punctuation,\n",
    "                preprocessing.strip_multiple_whitespaces,\n",
    "                preprocessing.strip_numeric,\n",
    "                preprocessing.remove_stopwords,\n",
    "                preprocessing.strip_short,\n",
    "            ]\n",
    "\n",
    "    def _preprocess_string(self, string):\n",
    "        return preprocessing.preprocess_string(string, filters=self.filters)\n",
    "\n",
    "    def _encode_target(self, target):\n",
    "        return 1 if target == 'positive' else 0\n",
    "\n",
    "    def __call__(self, item):\n",
    "        if isinstance(item['data'], str):\n",
    "            # String\n",
    "            data = self._preprocess_string(item['data'])\n",
    "        else:\n",
    "            # Iterable\n",
    "            data = [self._preprocess_string(d) for d in item['data']]\n",
    "\n",
    "        if isinstance(item['target'], str):\n",
    "            # String\n",
    "            target = self._encode_target(item['target'])\n",
    "        else:\n",
    "            # Iterable\n",
    "            target = [self._encode_target(t) for t in item['target']]\n",
    "\n",
    "        return {\n",
    "            'data': data,\n",
    "            'target': target\n",
    "        }\n",
    "\n",
    "preprocess = TextPreprocess()\n",
    "# Let's see the results\n",
    "preprocess(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Conversión a vectores\n",
    "\n",
    "Es posible continuar con la conversión del texto, a una representación por vectores. Si bien hay muchas posibilidades (siendo la *bolsa de palabras* una de las más utilizadas), en general para *Deep Learning* se prefieren representaciones utilizando vectores continuos, obtenidos por algún método del estilo de **Word2vec**, **Glove** o **FastText**. Para este caso utilizaremos las representaciones de *Glove* de dimensión 50 que se dejaron para descargar en el [Notebook 0](0_Set_Up.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[-0.18105   , -0.79229999, -0.097616  , ...,  1.42859995,\n",
       "         -0.032471  ,  0.47235999],\n",
       "        [ 0.69395   ,  0.69261003, -0.21608   , ...,  0.2247    ,\n",
       "         -0.23197   ,  0.0062523 ],\n",
       "        [-0.0049087 ,  0.12611   ,  0.14056   , ..., -0.58464003,\n",
       "         -0.31830999,  0.31564   ],\n",
       "        ...,\n",
       "        [ 0.25435999, -0.44304001, -0.12524   , ...,  0.73352998,\n",
       "          0.026198  ,  0.30408001],\n",
       "        [-0.058468  ,  0.019087  ,  0.089056  , ..., -0.28176001,\n",
       "          0.045137  , -0.18802001],\n",
       "        [ 0.14443   ,  0.39103001, -0.93454999, ..., -0.71325999,\n",
       "         -0.54575998,  0.13952   ]]),\n",
       " 'target': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorizeText:\n",
    "    def __init__(self, glove_vectors_path):\n",
    "        self.glove_model = KeyedVectors.load_word2vec_format('data/glove.6B.50d.txt', binary=False, no_header=True)\n",
    "        self.unknown_vector = np.random.randn(self.glove_model.vector_size) # Random vector for unknown words.\n",
    "\n",
    "    def _get_vector(self, word):\n",
    "        if word in self.glove_model:\n",
    "            # We know the embedding!\n",
    "            return self.glove_model[word]\n",
    "        else:\n",
    "            # Unknown word!\n",
    "            return self.unknown_vector\n",
    "\n",
    "    def _get_vectors(self, sentence):\n",
    "        return np.vstack([self._get_vector(word) for word in sentence])\n",
    "\n",
    "    def __call__(self, item):\n",
    "        review = []\n",
    "        if isinstance(item['data'][0], str):\n",
    "            # String\n",
    "            review = self._get_vectors(item['data'])\n",
    "        else:\n",
    "            # Iterable\n",
    "            review = [self._get_vectors(d) for d in item['data']]\n",
    "\n",
    "        return {\n",
    "            'data': review,\n",
    "            'target': item['target']\n",
    "        }\n",
    "\n",
    "vectorizer = VectorizeText('data/glove.6B.50d.txt.gz')\n",
    "# Let's see the results\n",
    "vectorizer(preprocess(dataset[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Combinación de vectores\n",
    "\n",
    "Si bien ahora estamos con una versión de los atributos que podría pasar por una red neuronal, hay un problema. Las distintas reseñas tienen diferentes largos, y como el algoritmo se entrena en lotes (*mini-batches*), estas deberían tener el mismo largo. Hay varias maneras de lidiar con esto, cada una con sus ventajas y desventajas. Dado que por ahora solo vimos *perceptrón multicapa* (**MLP**), que espera algo de tamaño fijo, una opción sencilla puede ser la de promediar el tamaño de los vectores de palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([ 0.10228207,  0.01455134, -0.1121626 , -0.27668046,  0.27284423,\n",
       "         0.06560054, -0.05191655, -0.04326527, -0.15702042,  0.19189558,\n",
       "        -0.21519975, -0.1007045 , -0.13937947, -0.00704294,  0.28695519,\n",
       "         0.02225536,  0.02181543, -0.01945881, -0.12781747, -0.25174534,\n",
       "        -0.06460028,  0.30616061,  0.14370052,  0.19194541,  0.09540124,\n",
       "        -1.08034224, -0.36215775,  0.17903577,  0.42573122, -0.18066451,\n",
       "         2.04714924,  0.15924272, -0.05178646, -0.40460137, -0.07660328,\n",
       "         0.10285706, -0.06545683, -0.16149122, -0.20091396, -0.24677175,\n",
       "        -0.13023138,  0.07413312,  0.04403049,  0.2685901 ,  0.14540364,\n",
       "         0.00686889, -0.00328018, -0.13658369, -0.05233869, -0.06472035]),\n",
       " 'target': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WordVectorsAverage:\n",
    "    def __call__(self, item):\n",
    "        if item['data'][0].ndim == 2:\n",
    "            data = np.vstack([np.mean(d, axis=0) for d in item['data']])\n",
    "        else:\n",
    "            data = np.mean(item['data'], axis=0)\n",
    "\n",
    "        return {\n",
    "            'data': data,\n",
    "            'target': item['target']\n",
    "        }\n",
    "\n",
    "vector_average = WordVectorsAverage()\n",
    "# Let's see the results\n",
    "vector_average(vectorizer(preprocess(dataset[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Conversión de vectores a tensores\n",
    "\n",
    "En el paso final, debemos convertir nuestros datos de arreglos de `numpy` a tensores de *PyTorch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': tensor([ 0.1023,  0.0146, -0.1122, -0.2767,  0.2728,  0.0656, -0.0519, -0.0433,\n",
       "         -0.1570,  0.1919, -0.2152, -0.1007, -0.1394, -0.0070,  0.2870,  0.0223,\n",
       "          0.0218, -0.0195, -0.1278, -0.2517, -0.0646,  0.3062,  0.1437,  0.1919,\n",
       "          0.0954, -1.0803, -0.3622,  0.1790,  0.4257, -0.1807,  2.0471,  0.1592,\n",
       "         -0.0518, -0.4046, -0.0766,  0.1029, -0.0655, -0.1615, -0.2009, -0.2468,\n",
       "         -0.1302,  0.0741,  0.0440,  0.2686,  0.1454,  0.0069, -0.0033, -0.1366,\n",
       "         -0.0523, -0.0647], dtype=torch.float64),\n",
       " 'target': tensor(1)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ToTensor:\n",
    "    def __call__(self, item):\n",
    "        \"\"\"\n",
    "        This espects a single array.\n",
    "        \"\"\"\n",
    "        return {'data': torch.from_numpy(item['data']), 'target': torch.tensor(item['target'])}\n",
    "\n",
    "to_tensor = ToTensor()\n",
    "# Let's see the results\n",
    "to_tensor(vector_average(vectorizer(preprocess(dataset[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Componiendo las transformaciones\n",
    "\n",
    "Para evitar tener que llamar a todas las funciones de transformación que queremos aplicar, hacemos uso del parámetro `transform` que definimos en nuestro `Dataset` (con un poco de ayuda de `functools`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 50000 elements\n",
      "Sample element...\n",
      "{'data': tensor([ 0.1023,  0.0146, -0.1122, -0.2767,  0.2728,  0.0656, -0.0519, -0.0433,\n",
      "        -0.1570,  0.1919, -0.2152, -0.1007, -0.1394, -0.0070,  0.2870,  0.0223,\n",
      "         0.0218, -0.0195, -0.1278, -0.2517, -0.0646,  0.3062,  0.1437,  0.1919,\n",
      "         0.0954, -1.0803, -0.3622,  0.1790,  0.4257, -0.1807,  2.0471,  0.1592,\n",
      "        -0.0518, -0.4046, -0.0766,  0.1029, -0.0655, -0.1615, -0.2009, -0.2468,\n",
      "        -0.1302,  0.0741,  0.0440,  0.2686,  0.1454,  0.0069, -0.0033, -0.1366,\n",
      "        -0.0523, -0.0647], dtype=torch.float64), 'target': tensor(1)}\n"
     ]
    }
   ],
   "source": [
    "def compose(*functions):\n",
    "    return functools.reduce(lambda f, g: lambda x: g(f(x)), functions, lambda x: x)\n",
    "\n",
    "dataset = IMDBReviewsDataset(\n",
    "    'data/imdb_reviews.csv.gz',\n",
    "    transform=compose(preprocess, vectorizer, vector_average, to_tensor)\n",
    ")\n",
    "\n",
    "print(f'Dataset loaded with {len(dataset)} elements')\n",
    "print(f'Sample element...\\n{dataset[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Iterando el conjunto de datos\n",
    "\n",
    "Ya tenemos nuestro conjunto de datos definitivo con sus respectivas transformaciones. ¿Para qué nos sirve esto? Una opción es simplemente iterar en el conjunto de datos de a un elemento por vez. Esto es sencillo, simplemente se hace a través de un `for`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1023,  0.0146, -0.1122, -0.2767,  0.2728,  0.0656, -0.0519, -0.0433,\n",
      "        -0.1570,  0.1919, -0.2152, -0.1007, -0.1394, -0.0070,  0.2870,  0.0223,\n",
      "         0.0218, -0.0195, -0.1278, -0.2517, -0.0646,  0.3062,  0.1437,  0.1919,\n",
      "         0.0954, -1.0803, -0.3622,  0.1790,  0.4257, -0.1807,  2.0471,  0.1592,\n",
      "        -0.0518, -0.4046, -0.0766,  0.1029, -0.0655, -0.1615, -0.2009, -0.2468,\n",
      "        -0.1302,  0.0741,  0.0440,  0.2686,  0.1454,  0.0069, -0.0033, -0.1366,\n",
      "        -0.0523, -0.0647], dtype=torch.float64)\n",
      "tensor(1)\n",
      "==================================================\n",
      "tensor([ 0.0767,  0.1736, -0.3908, -0.1270,  0.3595,  0.1924, -0.2518, -0.1332,\n",
      "        -0.1555,  0.3660, -0.0880,  0.2526, -0.1314,  0.1429,  0.2969, -0.0450,\n",
      "         0.0940,  0.0859, -0.1270, -0.3729,  0.0754,  0.3559,  0.0260,  0.0445,\n",
      "         0.3602, -0.7753, -0.5883,  0.1104,  0.3951, -0.1837,  2.1237, -0.0937,\n",
      "         0.0923, -0.4264,  0.0245,  0.3073, -0.0416,  0.2610, -0.2185, -0.3416,\n",
      "         0.1346,  0.1324, -0.1528,  0.0578, -0.0143,  0.0782,  0.2037, -0.0964,\n",
      "        -0.2085,  0.1937], dtype=torch.float64)\n",
      "tensor(1)\n",
      "==================================================\n",
      "tensor([ 0.1156,  0.1620, -0.2226, -0.2110,  0.4136,  0.2142, -0.2768, -0.1253,\n",
      "        -0.2487,  0.3100, -0.1870,  0.1175, -0.2071,  0.2026,  0.3963, -0.0145,\n",
      "         0.1064,  0.1892, -0.3575, -0.2874, -0.0762,  0.4070,  0.1601,  0.1652,\n",
      "         0.3301, -1.0604, -0.4467,  0.1917,  0.4583, -0.2003,  1.9967,  0.2179,\n",
      "         0.0460, -0.2379,  0.0279,  0.1072, -0.1289,  0.1671, -0.2856, -0.4432,\n",
      "        -0.0877,  0.1785, -0.0697,  0.0857,  0.1390,  0.0088,  0.0788, -0.3161,\n",
      "        -0.1101,  0.2739])\n",
      "tensor(1)\n",
      "==================================================\n",
      "tensor([ 0.1806, -0.0157, -0.1710, -0.2142,  0.3150,  0.3054, -0.3135, -0.0411,\n",
      "        -0.2538,  0.3440, -0.1863,  0.2306, -0.1899,  0.3741,  0.7131,  0.0028,\n",
      "         0.1438,  0.1538, -0.2893, -0.0805,  0.0642,  0.4769,  0.1579,  0.3752,\n",
      "         0.4151, -1.0463, -0.4686,  0.0356,  0.3485, -0.3394,  1.8880,  0.1231,\n",
      "        -0.0616, -0.3023, -0.0772,  0.2460, -0.0455, -0.1613, -0.0691, -0.5809,\n",
      "        -0.2408,  0.2757, -0.1319, -0.0316, -0.0288,  0.0371,  0.2117, -0.4086,\n",
      "        -0.0172,  0.2853])\n",
      "tensor(0)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for idx, sample in enumerate(dataset):\n",
    "    print(sample['data'])\n",
    "    print(sample['target'])\n",
    "    print('=' * 50)\n",
    "\n",
    "    if idx == 3:\n",
    "        # We don't need so many examples...\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## La clase `Dataloader`\n",
    "\n",
    "El problema con iterar de a un elemento por vez es que estamos limitados al querer entrenar un modelo. Para empezar, los modelos de *Deep Learning* suelen ser más eficientes si se entrenan utilizando algún tipo de entrenamiento por *mini-batches*. Además, hay otras cuestiones como mezclar los elementos (*shuffling*) o cargar datos en paralelo vía distintos *multiprocess workers*. La clase [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) precisamente se encarga de hacer eso por nosotros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 50]) torch.Size([4])\n",
      "1 torch.Size([4, 50]) torch.Size([4])\n",
      "2 torch.Size([4, 50]) torch.Size([4])\n",
      "3 torch.Size([4, 50]) torch.Size([4])\n",
      "tensor([[ 3.0416e-02,  1.4680e-01,  4.8470e-02, -2.3265e-01,  4.3905e-01,\n",
      "          1.0492e-01, -4.6799e-01, -1.1880e-01, -2.5818e-01,  2.0054e-01,\n",
      "         -1.8987e-01,  2.6144e-01, -1.7447e-01,  1.4546e-01,  7.0066e-01,\n",
      "          1.7029e-01,  1.3352e-01,  1.6263e-01, -3.0031e-01, -3.3626e-01,\n",
      "          1.6313e-02,  3.6978e-01,  2.9381e-01,  2.5543e-01,  4.1670e-01,\n",
      "         -1.2412e+00, -5.8719e-01,  1.1016e-01,  3.5761e-01, -3.9062e-01,\n",
      "          2.3470e+00,  2.7216e-01,  4.3461e-02,  2.5046e-02,  2.4115e-02,\n",
      "          5.8554e-02, -2.5833e-02, -8.1999e-03,  3.2247e-02, -4.1884e-01,\n",
      "         -1.3350e-01,  2.9274e-01, -2.3388e-02,  7.7495e-02,  8.9617e-02,\n",
      "          1.2941e-01,  2.0797e-02, -2.7418e-01, -7.5235e-02,  2.6937e-01],\n",
      "        [-6.2870e-02,  1.4123e-01, -2.9085e-01, -2.9693e-01,  2.9440e-01,\n",
      "          3.6957e-01, -1.9028e-01, -1.8564e-01, -2.2398e-01,  2.0754e-01,\n",
      "         -9.1265e-02,  4.3202e-02, -2.1095e-02,  1.6257e-01,  3.7469e-01,\n",
      "          5.3491e-02,  3.1449e-02,  1.1397e-01, -1.3595e-01, -4.9081e-01,\n",
      "         -2.6103e-01,  4.3329e-01,  2.3447e-01,  1.8013e-01,  2.2080e-01,\n",
      "         -8.7824e-01, -6.0582e-01,  4.0411e-01,  4.4778e-01, -4.7381e-01,\n",
      "          2.0636e+00,  1.9614e-01,  5.1062e-02, -8.9840e-02,  3.6886e-02,\n",
      "          2.0976e-01,  7.5664e-02, -9.7648e-02, -4.1387e-02, -4.4290e-01,\n",
      "         -1.3254e-01,  1.3640e-01,  1.9980e-03,  1.4063e-01,  2.6062e-01,\n",
      "         -1.2756e-01,  1.5388e-01, -4.0719e-01, -5.4814e-02,  1.1297e-01],\n",
      "        [ 1.5620e-01,  9.1329e-02, -9.0011e-02, -1.3524e-01,  2.8207e-01,\n",
      "          1.6027e-01, -3.5622e-01, -1.5529e-01, -1.1066e-01,  2.9700e-01,\n",
      "         -2.3535e-01,  8.8049e-02, -3.3951e-01,  2.3877e-01,  4.5841e-01,\n",
      "         -1.8935e-02,  1.7427e-01,  1.3418e-01, -2.9416e-01, -3.8537e-01,\n",
      "         -3.1812e-02,  2.7156e-01,  7.0698e-02,  2.4410e-01,  3.4110e-01,\n",
      "         -1.0136e+00, -6.1898e-01,  2.0882e-01,  4.7291e-01, -2.7607e-01,\n",
      "          2.1077e+00,  1.2827e-01,  1.3284e-02, -1.9224e-01,  5.6778e-02,\n",
      "          1.2148e-01, -5.5016e-02,  5.5211e-03, -1.7881e-01, -3.2779e-01,\n",
      "         -1.8197e-01,  1.7418e-01, -1.1122e-01,  2.1145e-02,  8.5338e-02,\n",
      "          2.4968e-02,  1.7304e-01, -2.2634e-01,  4.7560e-03,  2.2752e-01],\n",
      "        [ 3.0351e-02,  6.5218e-02, -2.1694e-01, -5.1061e-02,  2.0758e-01,\n",
      "          3.2597e-01, -1.7085e-01, -2.1223e-02, -4.1421e-03,  2.8509e-01,\n",
      "         -3.0166e-01,  1.8420e-01, -1.9165e-01, -1.1916e-02,  4.2853e-01,\n",
      "         -1.8102e-01,  2.8909e-01,  4.7278e-02, -1.6577e-01, -1.9332e-01,\n",
      "          1.3725e-01,  1.9340e-01,  4.9631e-02,  2.0121e-02,  1.4373e-01,\n",
      "         -5.8313e-01, -2.9914e-01,  1.0331e-01,  1.1910e-01, -1.2697e-01,\n",
      "          1.4433e+00, -2.5573e-01,  3.2937e-01, -2.1615e-01,  8.0175e-02,\n",
      "          2.8403e-01, -3.0735e-02,  8.4016e-02,  5.5470e-02, -2.9356e-01,\n",
      "         -1.1718e-02,  4.1346e-01, -1.8578e-01, -2.1428e-01, -5.1801e-02,\n",
      "          1.7117e-02,  4.3109e-02, -2.3029e-01, -1.4231e-01,  3.0092e-01]])\n",
      "tensor([0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, sample_batched['data'].size(), sample_batched['target'].size())\n",
    "\n",
    "    if i_batch == 3:\n",
    "        # We don't need so many examples...\n",
    "        print(sample_batched['data'])\n",
    "        print(sample_batched['target'])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## La clase `IterableDataset`\n",
    "\n",
    "El método preferido para trabajar con conjuntos de datos en *PyTorch* es `torch.utils.data.Dataset`. En general, hacer uso inteligente del método `__getitem__` es fundamental. Usándolo para cargar imágenes a medida que sean solicitadas y no al instanciar el dataset, es la mejor manera de trabajar con un conjunto de datos. En particular, de esta manera es mucho más fácil hacer *shuffling* de los datos y demás tareas. No obstante, esto no siempre es posible, muchas veces el conjunto de datos es demasiado grande para levantarlo en memoria (aunque sólo levantemos referencias). Para esos casos, *PyTorch* ofrece la clase [`torch.utils.data.IterableDataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset), donde el único método que es requerido implementar es `__iter__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch...\n",
      "{'data': ['Galoneira Semi Industrial', 'Máquina De Coser Brother Industrial', 'Teclado Casio Wk-240 76 Teclas Profissional Standard', 'Heladera Gafa 380 Impecable Urgente'], 'target': ['SEWING_MACHINES', 'SEWING_MACHINES', 'MUSICAL_KEYBOARDS', 'REFRIGERATORS']}\n"
     ]
    }
   ],
   "source": [
    "class MeLiChallengeDataset(IterableDataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.dataset_path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Since I have to work on a dataset that I CLEARLY don't have,\n",
    "        I had to forcibly enter my own code to reproduce this cell...\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(self.dataset_path)\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            item = {'data': row['title'], 'target': row['category']}\n",
    "\n",
    "            if self.transform:\n",
    "                yield self.transform(item)\n",
    "            else:\n",
    "                yield item\n",
    "\n",
    "dataset = MeLiChallengeDataset('data/dataset.csv')\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False, num_workers=0)\n",
    "dataiterable = iter(dataloader)\n",
    "print(f'Sample batch...\\n{dataiterable.next()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
