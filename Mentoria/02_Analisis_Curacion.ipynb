{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiploDatos 2021\n",
    "\n",
    "\n",
    "### Categorización de publicaciones de productos realizadas en Mercado Libre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 - Análisis y Curación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Condiciones generales que aplican a todos los prácticos:\n",
    "   - Las notebooks tienen que ser 100% reproducibles, es decir al ejecutar las celdas tal cuál como se entrega la notebook se deben obtener los mismos resultados sin errores.\n",
    "   - Código legible, haciendo buen uso de las celdas de la notebook y en lo posible seguir estándares de código para *Python* (https://www.python.org/dev/peps/pep-0008/).\n",
    "   - Utilizar celdas tipo *Markdown* para ir guiando el análisis.\n",
    "   - Limpiar el output de las celdas antes de entregar el notebook (ir a *Kernel* --> *Restart Kernel and Clear All Ouputs*).\n",
    "   - Incluir conclusiones del análisis que se hizo en la sección \"Conclusiones\". Tratar de aportar valor en esta sección, ser creativo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Consignas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sección A:  Limpieza de texto / Preprocessing\n",
    "\n",
    "Tener en cuenta lo siguiente: \n",
    "\n",
    "1. *Unidecode*\n",
    "\n",
    "2. Pasar a minúsculas\n",
    "\n",
    "3. Limpiar números\n",
    "\n",
    "4. Limpiar símbolos **(** ' ! ¡ \" @ % & * , . : ; < = > ¿ ? @ \\ ^ _ { | } ~ \\t \\n [ ] ` $ **)**\n",
    "\n",
    "5. Limpiar caracteres que suelen usarse como espacios **(** ' + ( ) - \\ **)**\n",
    "\n",
    "6. Reemplazar contracciones, por ejemplo, **c/u** por *cada uno*, **c/** por *con*, **p/** por *para*.\n",
    "\n",
    "7. Etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sección B: Tokenización & Secuencias\n",
    "\n",
    "1. Utilizar métodos `fit_on_texts()`, `texts_to_sequences()`, y `pad_sequences()`:\n",
    "\n",
    "- https://keras.io/api/preprocessing/text/#tokenizer-class\n",
    "\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sección C: Label Encoding\n",
    "\n",
    "1. Utilizar método `LabelEncoder()` de *sklearn*:\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sección D: Word Embeddings\n",
    "\n",
    "Generar los *word embeddings* correspondientes, de las siguientes dos formas:\n",
    "\n",
    "1. *Custom Word Embeddings*\n",
    "2. *Loading Pretrained Word Embeddings* (**opcional**)\n",
    "\n",
    "En ambos puntos el objetivos final es llegar a crear la *embedding layer* de *keras*:\n",
    "\n",
    "- https://keras.io/api/layers/core_layers/embedding/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Código y Análisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importaciones necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura de dataset reducido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset = pd.read_csv('DataSet/dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estudiamos el dataset brevemente antes de comenzar a operar sobre el mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.sort(df_dataset.category.unique())\n",
    "\n",
    "print(f'Dimensiones: {df_dataset.shape}')\n",
    "print('----------')\n",
    "print(f'Variables: {list(df_dataset.columns)}')\n",
    "print('----------')\n",
    "print(f'Categorías: {list(classes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sección A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de aplicar la limpieza, demos un vistazo a algunas de las publicaciones de nuestro conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.sample(10, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aplicamos Limpieza**\n",
    "\n",
    "Se define la serie de operaciones para la limpieza de títulos de publicaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(title):\n",
    "    \"\"\"\n",
    "    Aplica las operaciones de limpieza a un título de una publicación.\n",
    "    \"\"\"\n",
    "    # Unidecode: Convierte string de Unicode a ASCII.\n",
    "    title = unidecode(title)\n",
    "    # Pasamos a Minúsculas.\n",
    "    title = title.lower()\n",
    "    # Eliminamos Números.\n",
    "    title = re.sub(r'[0-9]+', '', title)\n",
    "    # Reemplazamos Contracciones.\n",
    "    title = re.sub(r'c/u', 'cada uno', title)\n",
    "    title = re.sub(r'c/', 'con', title)\n",
    "    title = re.sub(r'p/', 'para', title)\n",
    "    # Limpiamos Símbolos.\n",
    "    title = re.sub('[^a-zA-Z ]', '', title)\n",
    "    # Retornamos el título de la publicación procesado.\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset['clean_title'] = df_dataset.title.apply(cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limpieza Definitiva**\n",
    "\n",
    "Damos un vistazo al resultado del procesamiento, luego de haber aplicado todos los pasos anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset.sample(10, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observación sobre Unidecode**\n",
    "\n",
    "A simple vista, se eliminan los tildes (en ambos idiomas).\n",
    "\n",
    "Desde la [documentación](https://pypi.org/project/Unidecode/), se especifica:\n",
    "\n",
    "It often happens that you have text data in *Unicode*, but you need to represent it in *ASCII*.\n",
    "\n",
    "What **Unidecode** provides is a middle road: the function `unidecode()` takes *Unicode* data and tries to represent it in *ASCII* characters (i.e., the universally displayable characters between `0x00` and `0x7F`), where the compromises taken when mapping between two character sets are chosen to be near what a human with a *US* keyboard would choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sección B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separamos nuestro conjunto de datos en los vectores `X`, e `y`.\n",
    "\n",
    "- El primero, `X`, comprende los títulos procesados de las publicaciones.\n",
    "\n",
    "- El segundo, `y`, representa las categorías de las publicaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X = df_dataset.clean_title.values\n",
    "y = df_dataset.category.values\n",
    "\n",
    "X[0], y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos `Tokenizer()` para convertir los títulos de publicaciones en vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos conocer el tamaño de nuestro vocabulario (se suma `+ 1` para contemplar las palabras *out of vocabulary*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada palabra se transforma al correspondiente índice en nuestro vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_sentences = word_tokenizer.texts_to_sequences(X)\n",
    "\n",
    "embedded_sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos *padding* para que todos los vectores de palabras tengan tamaños equivalentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sentences = pad_sequences(embedded_sentences, padding='post')\n",
    "\n",
    "padded_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ammount_sentences, sentences_length = padded_sentences.shape\n",
    "\n",
    "ammount_sentences, sentences_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sección C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Necesitamos codificar las categorías de nuestras publicaciones.\n",
    "Por lo tanto, utilizamos `LabelEncoder()` para transformar los nombres en valores numéricos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuántas categorías identificó el *encoder*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le.transform(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos la transformación aprendida a todas las categorías de nuestro conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = le.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sección D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, solo restan obtener los *word embeddings* para los títulos de nuestras publicaciones.\n",
    "Utilizaremos `Embedding()` para calcularlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Word Embeddings**\n",
    "\n",
    "De manera arbitraria, los vectores resultantes serán de **25** dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "embedding_layer = Embedding(vocab_length, 25, input_length=sentences_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pretrained Word Embeddings**\n",
    "\n",
    "Desde https://nlp.stanford.edu/projects/glove/, se descarga el *word embeding* entrenado **glove.6B.zip**.\n",
    "\n",
    "De manera arbitraria, utilizaremos los vectores de **100** dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "glove_path = f'DataSet/glove.6B/glove.6B.{embedding_dim}d.txt'\n",
    "glove_file = open(glove_path, encoding='utf8')\n",
    "\n",
    "# Prepare embedding dictionary\n",
    "embeddings_dictionary = dict()\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "\n",
    "glove_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "print(f'Converted {hits} words ({misses} misses)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_embedding_layer = Embedding(vocab_length,\n",
    "                                    embedding_dim,\n",
    "                                    weights=[embedding_matrix],\n",
    "                                    input_length=sentences_length,\n",
    "                                    trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el laboratorio, nos concentramos principalmente en la curación de títulos de publicaciones en nuestro conjunto de datos, preparando la información para el aprendizaje de un futuro modelo.\n",
    "Por lo tanto, no contamos con demasiadas conclusiones sobre el procesamiento realizado.\n",
    "De todas formas, a continuación listaremos algunas observaciones interesantes.\n",
    "\n",
    "- La limpieza de títulos resulta una tarea compleja, y podría ser necesario regresar a esta etapa para refinarla.\n",
    "- El tamaño de nuestro vocabulario es **97180**. Hay que tener en cuenta que estamos trabajando con dos idiomas al mismo tiempo. Quizás podría ser necesario reducir su tamaño, limitándonos a las palabras más comunes.\n",
    "- Algunos parámetros fueron definidos de manera totalmente arbitraria, como las dimensiones de nuestro *word embedding* (**25**), o las dimensiones de los *word vectors* preentrenados (**100**).\n",
    "- Se utilizan los [GloVe](https://nlp.stanford.edu/projects/glove/) como nuestros *word vectors* preentrenados. Hay que notar que los vectores están preparados para trabajar con documentos en inglés (razón por la cual convertimos solo **28573** palabras, y perdemos **68606**). Una alternativa, sería utilizar [fastText](https://fasttext.cc/docs/en/pretrained-vectors.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendizaje Automático..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para practicar un poco con nuestra implementación, intentaremos predecir con lo que tenemos hasta este punto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definición del Modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(trained_embedding_layer)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(len(classes), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocesamiento**\n",
    "\n",
    "Necesitaríamos un paso adicional de procesamiento para utilizar nuestras categorías en el modelo definido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "one_hot_labels = to_categorical(encoded_labels, num_classes=len(classes))\n",
    "\n",
    "encoded_labels[0], one_hot_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamiento del Modelo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ya que este proceso puede demorar, realizaremos un entrenamiento breve\n",
    "model.fit(padded_sentences, one_hot_labels, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluamos contra nuestro propio conjunto de entrenamiento\n",
    "loss, accuracy = model.evaluate(padded_sentences, one_hot_labels, verbose=0)\n",
    "\n",
    "print(f'Accuracy: {accuracy * 100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicción**\n",
    "\n",
    "Se define un conjunto de datos de test improvisado, para analizar si el modelo aprende como nosotros esperamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(\n",
    "    [\n",
    "        'silla bebe auto',\n",
    "        'maquina cafe taza',\n",
    "        'cama dormir colchon',\n",
    "        'musica teclado teclas',\n",
    "        'jean pantalon talle',\n",
    "        'perro golden macho',\n",
    "        'heladera freezer frio',\n",
    "        'vino estancia uva'\n",
    "    ]\n",
    ")\n",
    "\n",
    "y_test = np.array(\n",
    "    [\n",
    "        'BABY_CAR_SEATS',\n",
    "        'COFFEE_MAKERS',\n",
    "        'MATTRESSES',\n",
    "        'MUSICAL_KEYBOARDS',\n",
    "        'PANTS',\n",
    "        'PUREBRED_DOGS',\n",
    "        'REFRIGERATORS',\n",
    "        'WINES'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos aplicar el mismo procesamiento del conjunto de entrenamiento, al conjunto improvisado de test (los títulos de las publicaciones no necesitan ser curados, ya que fueron definidos de forma tal para evitar este paso)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_sentences_test = word_tokenizer.texts_to_sequences(X_test)\n",
    "padded_sentences_test = pad_sequences(embedded_sentences_test, sentences_length, padding='post')\n",
    "\n",
    "embedded_sentences_test[0], padded_sentences_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels_test = le.transform(y_test)\n",
    "one_hot_labels_test = to_categorical(encoded_labels_test, num_classes=len(classes))\n",
    "\n",
    "encoded_labels_test[0], one_hot_labels_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intentamos predecir los datos improvisados\n",
    "predictions = model.predict(padded_sentences_test, verbose=0)\n",
    "\n",
    "print(f'Predicción - {list(np.argmax(predictions, axis=-1))}')\n",
    "print(f'Categorías - {list(encoded_labels_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvo las publicaciones de `MATTRESSES` y `MUSICAL_KEYBOARDS`, nuestro modelo predijo correctamente los datos de juguete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Material de ayuda para el desarrollo de este práctico:\n",
    "\n",
    "1. Implementación en *keras* de *word embeddings*: https://stackabuse.com/python-for-nlp-word-embeddings-for-deep-learning-in-keras\n",
    "2. Como utilizar *pre-trained word embeddings* en *keras*: https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "3. *Word Embeddings*: https://jalammar.github.io/illustrated-word2vec/\n",
    "3. Curso de **procesamiento del lenguaje natural** con *keras*: https://www.coursera.org/learn/natural-language-processing-tensorflow/home/welcome"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
