{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DiploDatos 2021\n",
    "\n",
    "\n",
    "### Categorización de publicaciones de productos realizadas en Mercado Libre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 - Análisis y Curación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Condiciones generales que aplican a todos los prácticos:\n",
    "   - Las notebooks tienen que ser 100% reproducibles, es decir al ejecutar las celdas tal cuál como se entrega la notebook se deben obtener los mismos resultados sin errores.\n",
    "   - Código legible, haciendo buen uso de las celdas de la notebook y en lo posible seguir estándares de código para Python (https://www.python.org/dev/peps/pep-0008/).\n",
    "   - Utilizar celdas tipo \"Markdown\" para ir guiando el análisis.\n",
    "   - Limpiar el output de las celdas antes de entregar el notebook (ir a Kernel --> Restart Kernel and Clear All Ouputs).\n",
    "   - Incluir conclusiones del análisis que se hizo en la sección \"Conclusiones\". Tratar de aportar valor en esta sección, ser creativo! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Consignas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sección A:  Limpieza de texto / Preprocessing\n",
    "\n",
    "Tener en cuenta lo siguiente: \n",
    "\n",
    "1. Unidecode\n",
    "\n",
    "2. Pasar a minúsculas\n",
    "\n",
    "3. Limpiar números\n",
    "\n",
    "4. Limpiar símbolos --> '!¡\"@$%&*,.:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "\n",
    "5. Limpiar caracteres que sueles usarse como espacios --> '+()-\\''\n",
    "\n",
    "6. Reemplazar \"contracciones\", por ejemplo:\n",
    "        -'c/u ' por 'cada uno'\n",
    "        -'c/' por 'con'\n",
    "        -'p/' por 'para'\n",
    "        \n",
    "        \n",
    "7. Etc! Pueden sumar mas steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sección B: Tokenización & Secuencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Utilizar métodos *fit_on_texts()*, *texts_to_sequences()* y *pad_sequences()*:\n",
    "\n",
    "- https://keras.io/api/preprocessing/text/#tokenizer-class\n",
    "\n",
    "- https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sección C: Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Utilizar método *LabelEncoder()* de sklearn:\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sección D: Word Embeddings\n",
    "\n",
    "Generar los word embeddings correspondientes, de las siguientes 2 formas: \n",
    "\n",
    "1. Custom Word Embeddings\n",
    "2. Loading Pretrained Word Embeddings (opcional)\n",
    "\n",
    "En ambos puntos el objetivos final es llegar a crear la embedding layer de keras:\n",
    "\n",
    "- https://keras.io/api/layers/core_layers/embedding/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Código y análisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura de dataset reducido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dataset =  pd.read_csv('DataSet/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trimmer', 'detailer', 'wahl', '+', 'kit', 'tijeras', 'stylecut']\n"
     ]
    }
   ],
   "source": [
    "# TO DO\n",
    "# 1 limpieza\n",
    "#Pasamos todo a minuscula\n",
    "df_dataset['title'] = df_dataset['title'].str.lower()\n",
    "#Limpiamos numeros\n",
    "for n in df_dataset['title']:\n",
    "    word= n.split()\n",
    "print(word[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-98be742dd452>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# dividimos palabras por un espacio en blanco\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5138\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'split'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Conclusiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Material de ayuda para el desarrollo de este práctico:\n",
    "\n",
    "1. Implementación en keras de word embeddings: https://stackabuse.com/python-for-nlp-word-embeddings-for-deep-learning-in-keras\n",
    "2. Como utilizar pre-trained word embeddings en Keras: https://keras.io/examples/nlp/pretrained_word_embeddings/\n",
    "3. Word Embeddings: https://jalammar.github.io/illustrated-word2vec/\n",
    "3. Curso de procesamiento del lenguaje natural con Keras: https://www.coursera.org/learn/natural-language-processing-tensorflow/home/welcome"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
