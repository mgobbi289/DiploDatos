{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Construyendo una red convolucional con PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gzip\n",
    "import mlflow\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.parsing import preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, average_precision_score\n",
    "\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Red convolucional para imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Datos del CIFAR10\n",
    "\n",
    "Utilizamos los mismos datos que se usaron en el [Notebook 1](1_Basic_MLP.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "CIFAR_CLASSES = ('plane',\n",
    "                 'car',\n",
    "                 'bird',\n",
    "                 'cat',\n",
    "                 'deer', \n",
    "                 'dog',\n",
    "                 'frog',\n",
    "                 'horse',\n",
    "                 'ship',\n",
    "                 'truck')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Red convolucional\n",
    "\n",
    "- La red convolucional se obtiene apilando capas [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).\n",
    "    - En particular, este tipo de capas acepta matrices (a diferencia de la lineal que sólo acepta vectores). En las capas se definen los canales de entrada y los de salida, además del tamaño del *kernel* (i.e. ventana).\n",
    "- También son comunes las capas [`torch.nn.MaxPool2d`](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) que realizan una operación de *max pooling*, en 2 dimensiones.\n",
    "- La red se completa con algunas capas lineales para poder llevarla a las 10 dimensiones de salida que vienen a representar las clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = CNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Entrenamiento\n",
    "\n",
    "La red se entrena igual que en el caso del *perceptrón multicapa* (**MLP**), solo que esta vez no requiere reacomodar la matriz de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001)\n",
    "\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23d03b5ba43480cb314322425cdedfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc1221fe9bd4da8986eb1a81f9f2e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Loss: NaN:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db9ee24466ba4a85ad2a5bfc41575e91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Loss: NaN:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39be32fea39c4bdf933f70c30d7d12e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Loss: NaN:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "iters_per_epoch = len(trainloader)\n",
    "for epoch in trange(EPOCHS): # Loop over the dataset multiple times...\n",
    "    pbar = tqdm(trainloader, desc='Train Loss: NaN')\n",
    "    for inputs, labels in pbar:\n",
    "        # Mini-Batch\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Train Loss Log\n",
    "        pbar.set_description(f'Train Loss: {loss.item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluación\n",
    "\n",
    "Una vez más, la evaluación es similar al caso del *perceptrón multicapa*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f3e1eb699944ba1910de5cbb4483573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       plane       0.59      0.51      0.54      1000\n",
      "         car       0.59      0.73      0.65      1000\n",
      "        bird       0.48      0.38      0.42      1000\n",
      "         cat       0.38      0.28      0.32      1000\n",
      "        deer       0.53      0.36      0.43      1000\n",
      "         dog       0.40      0.56      0.47      1000\n",
      "        frog       0.56      0.65      0.60      1000\n",
      "       horse       0.61      0.56      0.59      1000\n",
      "        ship       0.62      0.65      0.63      1000\n",
      "       truck       0.52      0.62      0.57      1000\n",
      "\n",
      "    accuracy                           0.53     10000\n",
      "   macro avg       0.53      0.53      0.52     10000\n",
      "weighted avg       0.53      0.53      0.52     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(testloader):\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.numpy())\n",
    "        y_pred.extend(predicted.numpy())\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=CIFAR_CLASSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CNNs para Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Datos IMDB\n",
    "\n",
    "Similar al caso de **CNN** para imágenes, vamos a volver sobre el conjunto de datos que ya utilizamos anteriomente: el de reseñas **IMDB**. Esta vez para compararlo contra el modelo del *perceptrón multicapa* utilizando la media de los embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class IMDBReviewsDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if torch.is_tensor(item):\n",
    "            item = item.to_list()\n",
    "\n",
    "        item = {\n",
    "            'data': self.dataset.loc[item, 'review'],\n",
    "            'target': self.dataset.loc[item, 'sentiment']\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Preprocesamiento\n",
    "\n",
    "Aplicamos el mismo tipo de preprocesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class RawDataProcessor:\n",
    "    def __init__(self, dataset, ignore_header=True, filters=None, vocab_size=50000):\n",
    "        if filters:\n",
    "            self.filters = filters\n",
    "        else:\n",
    "            self.filters = [\n",
    "                lambda s: s.lower(),\n",
    "                preprocessing.strip_tags,\n",
    "                preprocessing.strip_punctuation,\n",
    "                preprocessing.strip_multiple_whitespaces,\n",
    "                preprocessing.strip_numeric,\n",
    "                preprocessing.remove_stopwords,\n",
    "                preprocessing.strip_short,\n",
    "            ]\n",
    "\n",
    "        # Create dictionary based on all the reviews (with corresponding preprocessing).\n",
    "        self.dictionary = corpora.Dictionary(\n",
    "            dataset['review'].map(self._preprocess_string).tolist()\n",
    "        )\n",
    "        # Filter the dictionary and compactify it (make the indices continous).\n",
    "        self.dictionary.filter_extremes(no_below=2, no_above=1, keep_n=vocab_size)\n",
    "        self.dictionary.compactify()\n",
    "        # Add a couple of special tokens.\n",
    "        self.dictionary.patch_with_special_tokens({'[PAD]': 0, '[UNK]': 1})\n",
    "        self.idx_to_target = sorted(dataset['sentiment'].unique())\n",
    "        self.target_to_idx = {t: i for i, t in enumerate(self.idx_to_target)}\n",
    "\n",
    "    def _preprocess_string(self, string):\n",
    "        return preprocessing.preprocess_string(string, filters=self.filters)\n",
    "\n",
    "    def _sentence_to_indices(self, sentence):\n",
    "        return self.dictionary.doc2idx(sentence, unknown_word_index=1)\n",
    "\n",
    "    def encode_data(self, data):\n",
    "        return self._sentence_to_indices(self._preprocess_string(data))\n",
    "\n",
    "    def encode_target(self, target):\n",
    "        return self.target_to_idx[target]\n",
    "\n",
    "    def __call__(self, item):\n",
    "        if isinstance(item['data'], str):\n",
    "            # String\n",
    "            data = self.encode_data(item['data'])\n",
    "        else:\n",
    "            # Iterable\n",
    "            data = [self.encode_data(d) for d in item['data']]\n",
    "\n",
    "        if isinstance(item['target'], str):\n",
    "            # String\n",
    "            target = self.encode_target(item['target'])\n",
    "        else:\n",
    "            # Iterable\n",
    "            target = [self.encode_target(t) for t in item['target']]\n",
    "\n",
    "        return {'data': data, 'target': target}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/imdb_reviews.csv.gz')\n",
    "\n",
    "preprocess = RawDataProcessor(dataset)\n",
    "\n",
    "train_indices, test_indices = train_test_split(dataset.index, test_size=0.2, random_state=123)\n",
    "\n",
    "train_dataset = IMDBReviewsDataset(dataset.loc[train_indices].reset_index(drop=True), transform=preprocess)\n",
    "test_dataset = IMDBReviewsDataset(dataset.loc[test_indices].reset_index(drop=True), transform=preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Padding de secuencias\n",
    "\n",
    "Dado que en este caso utilizaremos las secuencias completas sobre las que aplicaremos las convoluciones, necesitamos trabajar con dichas secuencias de manera que en un *batch* de datos se tenga el tamaño correcto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class PadSequences:\n",
    "    def __init__(self, pad_value=0, max_length=None, min_length=1):\n",
    "        assert max_length is None or min_length <= max_length\n",
    "        self.pad_value = pad_value\n",
    "        self.max_length = max_length\n",
    "        self.min_length = min_length\n",
    "\n",
    "    def __call__(self, items):\n",
    "        data, target = list(zip(*[(item['data'], item['target']) for item in items]))\n",
    "        seq_lengths = [len(d) for d in data]\n",
    "\n",
    "        if self.max_length:\n",
    "            max_length = self.max_length\n",
    "            seq_lengths = [min(self.max_length, l) for l in seq_lengths]\n",
    "        else:\n",
    "            max_length = max(self.min_length, max(seq_lengths))\n",
    "\n",
    "        data = [d[:l] + [self.pad_value] * (max_length - l) for d, l in zip(data, seq_lengths)]\n",
    "\n",
    "        return {'data': torch.LongTensor(data), 'target': torch.FloatTensor(target)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DataLoaders\n",
    "\n",
    "Una vez creada nuestra función para hacer *padding* de secuencia, definiremos los `DataLoaders`. Una cuestión importante, las redes convolucionales sobre texto esperan que todas las secuencias sean al menos del tamaño de la convolución máxima (caso contrario ocurrirá un error por no poder realizar la convolución sobre un espacio más chico que el tamaño del filtro). Es por eso que utilizamos el parámetro `min_length` esta vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "FILTERS_COUNT = 100\n",
    "\n",
    "FILTERS_LENGTH = [2, 3, 4]\n",
    "pad_sequences = PadSequences(min_length=max(FILTERS_LENGTH))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, collate_fn=pad_sequences, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, collate_fn=pad_sequences, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Red convolucional sobre texto\n",
    "\n",
    "Por último, tenemos la red convolucional sobre texto. Si bien arranca muy similar al caso del clasificador del *perceptrón multicapa*, vemos que en este caso hacemos uso de [`torch.nn.Conv1d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html) dado que sólo nos desplazamos por una dimensión (i.e. la secuencia). En particular, como utilizamos *max pooling* global, no hacemos uso del módulo `torch.nn` para calcularlo, simplemente utilizamos el método `max()` del tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class IMDBReviewsClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_embeddings_path, dictionary, vector_size, freeze_embedings):\n",
    "        super().__init__()\n",
    "        embeddings_matrix = torch.randn(len(dictionary), vector_size)\n",
    "        embeddings_matrix[0] = torch.zeros(vector_size)\n",
    "        with gzip.open(pretrained_embeddings_path, 'rt') as fh:\n",
    "            for line in fh:\n",
    "                word, vector = line.strip().split(None, 1)\n",
    "                if word in dictionary.token2id:\n",
    "                    # We know the embedding!\n",
    "                    wordID = dictionary.token2id[word]\n",
    "                    embeddings_matrix[wordID] = torch.FloatTensor([float(n) for n in vector.split()])\n",
    "        # Embedding Layer\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings_matrix, freeze=freeze_embedings, padding_idx=0)\n",
    "        # Convolutional Layers\n",
    "        self.convs = []\n",
    "        for filter_lenght in FILTERS_LENGTH:\n",
    "            self.convs.append(nn.Conv1d(vector_size, FILTERS_COUNT, filter_lenght))\n",
    "        self.convs = nn.ModuleList(self.convs)\n",
    "        # Fully Connected Layers\n",
    "        self.fc = nn.Linear(FILTERS_COUNT * len(FILTERS_LENGTH), 128)\n",
    "        self.output = nn.Linear(128, 1)\n",
    "        self.vector_size = vector_size\n",
    "\n",
    "    @staticmethod\n",
    "    def conv_global_max_pool(x, conv):\n",
    "        return F.relu(conv(x).transpose(1, 2).max(1)[0])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x).transpose(1, 2) # Conv1d takes (batch, channel, seq_len)\n",
    "        x = [self.conv_global_max_pool(x, conv) for conv in self.convs]\n",
    "        x = torch.cat(x, dim=1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        x = torch.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Experimento\n",
    "\n",
    "El experimento de **MLFlow** es prácticamente igual, salvo que cambiamos algunos de los parámetros a guardar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'a_CNN_experiment' does not exist. Creating a new experiment\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a70b11f9d314f11bffa3d50eeab535d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f84f1cecbf4f5a88664a78e4a26575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714ef64c03814db0ab9f2fa494b1f36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da37bac1dd8048da89f76807a8c4dcf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a567290e879c48d4b99fa4041e816de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b7b19aa97a483897d3406b9a91e6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e2753c424e4b0ba1c2dd7f091edb26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c1535ab725467c8095b00a189c3b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlflow.set_experiment('a_CNN_experiment')\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # HyperParameters Logs\n",
    "    mlflow.log_param('model_name', 'CNN')\n",
    "    mlflow.log_param('freeze_embedding', True)\n",
    "    mlflow.log_params({\n",
    "        'filters_count': FILTERS_COUNT,\n",
    "        'filters_length': FILTERS_LENGTH,\n",
    "        'fc_size': 128\n",
    "    })\n",
    "    # Model Definition\n",
    "    model = IMDBReviewsClassifier('data/glove.6B.50d.txt.gz', preprocess.dictionary, 50, True)\n",
    "    loss = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001)\n",
    "    # Training\n",
    "    for epoch in trange(3):\n",
    "        model.train()\n",
    "        running_loss = []\n",
    "        for idx, batch in enumerate(tqdm(train_loader)):\n",
    "            # Mini-Batch Training\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch['data'])\n",
    "            loss_value = loss(output, batch['target'].view(-1, 1))\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            running_loss.append(loss_value.item())\n",
    "        # Train Loss for each epoch\n",
    "        mlflow.log_metric('train_loss', sum(running_loss) / len(running_loss), epoch)\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        targets = []\n",
    "        predictions = []\n",
    "        running_loss = []\n",
    "        for batch in tqdm(test_loader):\n",
    "            # Mini-Batch Evaluation\n",
    "            output = model(batch['data'])\n",
    "            loss_value = loss(output, batch['target'].view(-1, 1))\n",
    "            running_loss.append(loss_value.item())\n",
    "            targets.extend(batch['target'].numpy())\n",
    "            predictions.extend(output.squeeze().detach().numpy())\n",
    "        # Test Loss for each epoch\n",
    "        mlflow.log_metric('test_loss', sum(running_loss) / len(running_loss), epoch)\n",
    "        mlflow.log_metric('test_avp', average_precision_score(targets, predictions), epoch)\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "        targets = []\n",
    "        predictions = []\n",
    "        for batch in tqdm(test_loader):\n",
    "            output = model(batch['data'])\n",
    "            # Let's save the final predictions\n",
    "            targets.extend(batch['target'].numpy())\n",
    "            predictions.extend(output.squeeze().detach().numpy())\n",
    "        # Predictions DataFrame\n",
    "        results = {'prediction': predictions, 'target': targets}\n",
    "        pd.DataFrame(results).to_csv(f'{tmpdirname}/predictions.csv.gz', index=False)\n",
    "        # Predictions Log\n",
    "        mlflow.log_artifact(f'{tmpdirname}/predictions.csv.gz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
