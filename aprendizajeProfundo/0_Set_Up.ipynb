{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning - Part 0\n",
    "\n",
    "This notebook explains how to install all the prerequisites and libraries that you will need to run the following tutorials. If you can execute all the following cells, you are good to go.\n",
    "\n",
    "## Environment configuration\n",
    "\n",
    "### Install **conda**\n",
    "\n",
    "There are two major package managers in *Python*: **pip** and **conda**. For this tutorial we will be using *conda* which, besides being a package manager is also useful as a version manager. There are two main ways to install *conda*: **Anaconda** and **Miniconda**. Any of them will be useful for this course, just follow the instructions here, according to your operative system:\n",
    "\n",
    "https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html#regular-installation\n",
    "\n",
    "### Create an environment with all the *Anaconda* libraries\n",
    "\n",
    "`$ conda create --name deeplearning python=3.7 anaconda`\n",
    "\n",
    "Don't forget to activate the new environment...\n",
    "\n",
    "`$ conda activate deeplearning`\n",
    "\n",
    "### Install *PyTorch*\n",
    "\n",
    "This year we will be using [PyTorch](https://pytorch.org/) as the library to build and train the deep learning models. The library is a little less abstract than other possibilities, such as [Keras](https://www.tensorflow.org/guide/keras), but gives a little more control to the user which in turn allows more customization.\n",
    "\n",
    "In order to install **PyTorch** we recommend the following [official documentation](https://pytorch.org/get-started/locally/). In your local machine, you will install the version that only has *CPU* support (i.e. no *CUDA* version), but in *Nabucodonosor* you need to install the version with *GPU* support.\n",
    "\n",
    "#### CPU\n",
    "\n",
    "Install *PyTorch* for CPU:\n",
    "\n",
    "`(deeplearning) $ conda install pytorch torchvision cpuonly -c pytorch`\n",
    "\n",
    "Then just check the version installed is $>= 1.7.0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.1+cu102'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU\n",
    "\n",
    "The **GPU PyTorch** depends on the *CUDA* version installed. *Nabucodonosor* has many installations of *CUDA* in the `/opt/cuda` directory. You need to add `nvcc` to the `$PATH`. For example, to setup for *CUDA 10.2*, do the following:\n",
    "\n",
    "`(deeplearning) $ export PATH=/opt/cuda/10.2/bin:$PATH`\n",
    "\n",
    "That needs to be done every time you enter *Nabucodonosor*. To avoid that step, add it to your `.bashrc` file:\n",
    "\n",
    "`(deeplearning) $ echo \"export PATH=/opt/cuda/10.2/bin:$PATH\" >> $HOME/.bashrc`\n",
    "\n",
    "Then, install the *PyTorch* library:\n",
    "\n",
    "`(deeplearning) $ conda install pytorch torchvision cudatoolkit=10.2 -c pytorch`\n",
    "\n",
    "Check if this is working by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Google Colab\n",
    "\n",
    "In case you want to install *PyTorch* on a *Google Colab*, it is possible, but first you need to check what version of `nvcc` is running. For that run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\r\n",
      "Built on Fri_Feb__8_19:08:17_PST_2019\r\n",
      "Cuda compilation tools, release 10.1, V10.1.105\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to what the previous cell tells you, you will need to install the proper drivers, with `pip` instead of *conda*. Please refer to the [getting started](https://pytorch.org/get-started/locally/) page and check what to do next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install other libraries\n",
    "\n",
    "We need the `gensim` library to deal with *word embeddings*, so you need to install it. Plus, the `mlflow` tool to keep track of experiments. Finally, `tqdm` is a handful progress bar to keep track of different processes.\n",
    "\n",
    "`(deeplearning) $ conda install gensim mlflow tqdm -c conda-forge`\n",
    "\n",
    "If you have problems importing `gensim` and get this error:\n",
    "\n",
    "`ImportError: cannot import name 'open' from 'smart_open' (C:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\__init__.py)`\n",
    "\n",
    "Then try updating `smart_open`:\n",
    "\n",
    "`(deeplearning) $ conda update smart_open`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download *embeddings* and dataset\n",
    "\n",
    "### CIFAR10\n",
    "\n",
    "The dataset we will use (**CIFAR10**) is part of the `torchvision` package, which makes it fairly easy to download. You can learn more details on it [here](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#loading-and-normalizing-cifar10):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63712a054b42468b86c04c11ec766709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "torchvision.datasets.CIFAR10(root='data', download=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Glove Embeddings* and *IMDB Reviews* Dataset\n",
    "\n",
    "Some examples that we will run for *text classification* using **Convolutional Neural Networks** require the *Glove Embeddings* as well as the *IMDB Reviews* dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 65.9M  100 65.9M    0     0  32.7M      0  0:00:02  0:00:02 --:--:-- 32.7M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 25.3M  100 25.3M    0     0  30.2M      0 --:--:-- --:--:-- --:--:-- 30.1M\n"
     ]
    }
   ],
   "source": [
    "!curl -L https://cs.famaf.unc.edu.ar/\\~ccardellino/resources/diplodatos/glove.6B.50d.txt.gz -o data/glove.6B.50d.txt.gz\n",
    "!curl -L https://cs.famaf.unc.edu.ar/\\~ccardellino/resources/diplodatos/imdb_reviews.csv.gz -o data/imdb_reviews.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeLi Challenge Dataset\n",
    "\n",
    "For the course project, we will be using a dataset based on the **2019 MeLi Challenge** dataset, for automatic classification of products categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  945M  100  945M    0     0  34.9M      0  0:00:27  0:00:27 --:--:-- 35.0M\n",
      "meli-challenge-2019/\n",
      "meli-challenge-2019/spanish.test.jsonl.gz\n",
      "meli-challenge-2019/portuguese.validation.jsonl.gz\n",
      "meli-challenge-2019/portuguese.train.jsonl.gz\n",
      "meli-challenge-2019/spanish.train.jsonl.gz\n",
      "meli-challenge-2019/spanish_token_to_index.json.gz\n",
      "meli-challenge-2019/portuguese_token_to_index.json.gz\n",
      "meli-challenge-2019/spanish.validation.jsonl.gz\n",
      "meli-challenge-2019/portuguese.test.jsonl.gz\n"
     ]
    }
   ],
   "source": [
    "!curl -L https://cs.famaf.unc.edu.ar/\\~ccardellino/resources/diplodatos/meli-challenge-2019.tar.bz2 -o data/MeLi_Challenge.tar.bz2\n",
    "!tar jxvf data/MeLi_Challenge.tar.bz2 -C data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using *Nabucodonosor*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunneling and **ssh**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do you run a notebook in a remote machine?** You use an **ssh** connection with a port forwarding. This way, everything that goes to the port on the server machine (like a *jupyter notebook*) also goes to your *localhost*.\n",
    "\n",
    "It is likely that everyone will be using the same ports, so we recommend you to select a random number before connecting. The port on the **ssh** must be the same that you use to start the notebook.\n",
    "\n",
    "```\n",
    "$ ssh -L PORT:localhost:PORT USER@SERVER\n",
    "$ conda activate deeplearning\n",
    "(deeplearning) $ jupyter notebook --port PORT --no-browser\n",
    "```\n",
    "\n",
    "Now you can use the notebook as if it was running on your computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using **slurm**\n",
    "\n",
    "The *Nabucodonosor* server uses a queue system called **slurm**, which grants exclusive access to the GPU resources. You should enqueue everything you do that takes more than 10 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set-Up\n",
    "\n",
    "1. Download the script https://raw.githubusercontent.com/MIREL-UNC/mirel-scripts/master/run_scripts/submit_job_slurm.sh.\n",
    "\n",
    "2. Create a *logs* folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enqueue things\n",
    "\n",
    "To enqueue a job on **slurm**, first put your command in a file, for example `command.txt`.\n",
    "\n",
    "```\n",
    "$ sbatch submit_job_slurm.sh command.txt\n",
    "```\n",
    "\n",
    "The queue will assign your job a number **JOBID**. All the output of your process will be redirected to `logs/JOBID.out` and `logs/JOBID.err`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Controlling things\n",
    "\n",
    "To see the state of the queue run `$ squeue`, and to cancel a job run `$ scancel JOBID`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoid using GPUs\n",
    "\n",
    "If all the GPUs are being used, you can still force **PyTorch** to use the CPU. For simple models this is still a very good option.\n",
    "\n",
    "The easiest way is to set the environment variable `CUDA_VISIBLE_DEVICES=\"\"` when running your commands. For example:\n",
    "\n",
    "```\n",
    "(deeplearning) $ CUDA_VISIBLE_DEVICES=\"\" jupyter notebook --no-browser\n",
    "(deeplearning) $ CUDA_VISIBLE_DEVICES=\"\" exercise_1.py --experiment_name mlp_200\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
