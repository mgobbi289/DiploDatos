{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qy2yUgKC8vsg"
   },
   "source": [
    "# Búsqueda de Hiperparámetros\n",
    "\n",
    "Las redes neuronales tienen decenas de hiperparámetros que afectan su arquitectura y proceso de entrenamiento. Más aún, el desempeño final del modelo está condicionado a encontar un conjunto de valores para dichos hiperparámetros exitosos, para una inicialización aleatoria de los pesos dada. Por ello, la exploración de hiperparámetros se vuelve una de las partes más tediosas y críticas del entrenamiento de redes neuronales. Para obtener resultados que sean correctos, significativos, y reproducibles, es necesario planificar y sistemizar este proceso de búsqueda.\n",
    "\n",
    ">  Hyper-parameter optimization should be regarded as a formal outer loop in the learning process.\n",
    "\n",
    "Formalmente, este proceso se puede describir como la minimización de la función de pérdida (o la maximización de la performance) como si fuera una función de *caja negra* que toma como parámetros los valores de los hiperparámetros:\n",
    "\n",
    "$$ f(\\theta) = loss_\\theta(y, \\hat{y}) $$\n",
    "$$ \\theta^* = argmin_\\theta f(\\theta) $$\n",
    "\n",
    "donde $\\theta$ es el conjunto de hiperparámetros del modelo, $loss$ es la pérdida generada entre las etiquetas verdaderas $y$, y las etiquetas generadas por el modelo $\\hat{y}$, y $f$ es la función objetivo de la minimización.\n",
    "\n",
    "\n",
    "Las estrategias principales para la exploración del espacio de hiperparámetros son:\n",
    "* Búsqueda manual, donde un humano define los valores de cada hiperparámetro.\n",
    "* Búsqueda por grilla o *grid search*, donde se define un conjunto de valores posibles que puede tomar cada hiperparámetro, y se realiza un experimento por cada combinación posible.\n",
    "* Búsqueda aleatoria o *random search*, donde se define un rango de valores posibles para cada hiperparámetro, y se elige al azar un valor del rango para cada experimento.\n",
    "* Búsqueda automátizada, *automated search* o *model-based search*, que es igual a la búsqueda aleatoria pero la selección del valor de cada hiperparámetro está condicionado por los resultados de experimentos anteriores. Para más información ver el paper [*Algorithms for Hyper-Parameter Optimization*](https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf).\n",
    "\n",
    "En la siguiente imagen, tomada del paper [*Random Search for Hyper-Parameter Optimization*](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf), se muestra el impacto de las primeras dos estrategias para un hiperparámetro con alta influencia en el desempeño del modelo final, y otro sin influencia en el modelo. No solo requiere muchas evaluaciones para lograr cobertura, sino que las combinaciones en dónde sólo se varían hiperparámetros no relevantes no recolectan información nueva. El éxito de la búsqueda por grilla depende de que el nivel de granularidad de la grilla cubra adecuadamente los valores relevantes, que son desconocidos a priori.\n",
    "\n",
    "![Comparación de las exploraciones entre grid search y random search](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1531340388/grid_vs_random_jltknd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZnHi2iN7BiA"
   },
   "source": [
    "Para solucionar todos estos problemas, es que se utiliza la **exploración bayesiana**. Este método modela la pérdida como un *Gaussian Process*, y tiene en cuenta los resultados de los experimentos anteriores para ir construyendo una distribución de probabilidad de la pérdida dados los hiperparámetros:\n",
    "\n",
    "$$ P(loss | \\theta)$$\n",
    "\n",
    "Para elegir una nueva combinación de hiperparámetros a probar dados los experimentos previos, el algoritmo utiliza una *surrogate function* para aproximar el comportamiento de la pérdida, y una *selection function* basada en la mejora esperada. A grandes rasgos, el algoritmo sigue los siguientes pasos:\n",
    "\n",
    "  1. Encontrar el mejor conjunto de hiperparámetros que maximize la mejora esperada (**EI**), estimada a través de la *surrogate function*.\n",
    "  2. Calcular la performance del modelo con la combinación de hiperparámetros elegida. Esto corresponde a evaluar la función objetivo.\n",
    "  3. Actualizar la forma de la *surrogate function* utilizando el teorema de *Bayes* para que se ajuste mejor a la verdadera distribución $ P(loss | \\theta)$.\n",
    "\n",
    "Afortunadamente, muchos algoritmos de búsqueda están implementados y funcionan como cajas negras. Veremos un ejemplo utilizando la librería `hyperopt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VJmgwjm48vso"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn\n",
    "\n",
    "import csv\n",
    "import gzip\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.parsing import preprocessing\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "t0XhsOvd_nbC",
    "outputId": "8de096c8-c530-4861-b94b-9e3835af99b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.1.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure version 4.X!\n",
    "import gensim\n",
    "\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUzR6Nfv8vsp"
   },
   "source": [
    "## Parte 1: Preprocesamiento del texto\n",
    "\n",
    "Primero leeremos el dataset como se explica en la notebook `5_CNNs.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bVdiWbML8vsq"
   },
   "outputs": [],
   "source": [
    "class IMDBReviewsDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if torch.is_tensor(item):\n",
    "            item = item.to_list()\n",
    "\n",
    "        item = {\n",
    "            'data': self.dataset.loc[item, 'review'],\n",
    "            'target': self.dataset.loc[item, 'sentiment']\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "\n",
    "        return item\n",
    "\n",
    "class RawDataProcessor:\n",
    "    def __init__(self, dataset, ignore_header=True, filters=None, vocab_size=50000):\n",
    "        if filters:\n",
    "            self.filters = filters\n",
    "        else:\n",
    "            self.filters = [\n",
    "                lambda s: s.lower(),\n",
    "                preprocessing.strip_tags,\n",
    "                preprocessing.strip_punctuation,\n",
    "                preprocessing.strip_multiple_whitespaces,\n",
    "                preprocessing.strip_numeric,\n",
    "                preprocessing.remove_stopwords,\n",
    "                preprocessing.strip_short,\n",
    "            ]\n",
    "\n",
    "        # Create dictionary based on all the reviews (with corresponding preprocessing)\n",
    "        self.dictionary = corpora.Dictionary(\n",
    "            dataset['review'].map(self._preprocess_string).tolist()\n",
    "        )\n",
    "        # Filter the dictionary and compactify it (make the indices continous)\n",
    "        self.dictionary.filter_extremes(no_below=2, no_above=1, keep_n=vocab_size)\n",
    "        self.dictionary.compactify()\n",
    "        # Add a couple of special tokens\n",
    "        self.dictionary.patch_with_special_tokens({'[PAD]': 0, '[UNK]': 1})\n",
    "        self.idx_to_target = sorted(dataset['sentiment'].unique())\n",
    "        self.target_to_idx = {t: i for i, t in enumerate(self.idx_to_target)}\n",
    "\n",
    "    def _preprocess_string(self, string):\n",
    "        return preprocessing.preprocess_string(string, filters=self.filters)\n",
    "\n",
    "    def _sentence_to_indices(self, sentence):\n",
    "        return self.dictionary.doc2idx(sentence, unknown_word_index=1)\n",
    "\n",
    "    def encode_data(self, data):\n",
    "        return self._sentence_to_indices(self._preprocess_string(data))\n",
    "\n",
    "    def encode_target(self, target):\n",
    "        return self.target_to_idx[target]\n",
    "\n",
    "    def __call__(self, item):\n",
    "        if isinstance(item['data'], str):\n",
    "            # String\n",
    "            data = self.encode_data(item['data'])\n",
    "        else:\n",
    "            # Iterable\n",
    "            data = [self.encode_data(d) for d in item['data']]\n",
    "\n",
    "        if isinstance(item['target'], str):\n",
    "            # String\n",
    "            target = self.encode_target(item['target'])\n",
    "        else:\n",
    "            # Iterable\n",
    "            target = [self.encode_target(t) for t in item['target']]\n",
    "\n",
    "        return {'data': data, 'target': target, 'sentence': item['data']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N38F6o3VAYSD"
   },
   "source": [
    "### Separando el conjunto de validación o *dev*\n",
    "\n",
    "En aprendizaje profundo, es **MUY** importante utilizar un conjunto de validación durante la búsqueda de hiperparámetros, que puede ser tomado de la partición de entrenamiento. Esto es independiente de la estrategia de búsqueda que se utilice.\n",
    "\n",
    "De esta manera, se previene el overfitting indirecto y se cuenta con una partición de datos nunca antes vista para poder evaluar la generalización real del modelo a datos no vistos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fdxy4poX_8_3"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/imdb_reviews.csv.gz')\n",
    "\n",
    "preprocess = RawDataProcessor(dataset)\n",
    "\n",
    "# Train and Test\n",
    "train_indices, test_indices = train_test_split(dataset.index, test_size=0.2, random_state=123)\n",
    "# Train and Val\n",
    "train_indices, val_indices = train_test_split(train_indices, test_size=0.2, random_state=123)\n",
    "\n",
    "# Train\n",
    "train_dataset = IMDBReviewsDataset(dataset.loc[train_indices].reset_index(drop=True), transform=preprocess)\n",
    "# Val\n",
    "val_dataset = IMDBReviewsDataset(dataset.loc[val_indices].reset_index(drop=True), transform=preprocess)\n",
    "# Test: We won't use test_dataset until the end!\n",
    "test_dataset = IMDBReviewsDataset(dataset.loc[test_indices].reset_index(drop=True), transform=preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gemdFTfO8ELO"
   },
   "outputs": [],
   "source": [
    "class PadSequences:\n",
    "    def __init__(self, pad_value=0, max_length=100):\n",
    "        self.pad_value = pad_value\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, items):\n",
    "        data, target = list(zip(*[(item['data'], item['target']) for item in items]))\n",
    "        seq_lengths = [len(d) for d in data]\n",
    "\n",
    "        max_length = self.max_length\n",
    "        seq_lengths = [min(self.max_length, l) for l in seq_lengths]\n",
    "\n",
    "        data = [d[:l] + [self.pad_value] * (max_length - l) for d, l in zip(data, seq_lengths)]\n",
    "\n",
    "        return {'data': torch.LongTensor(data), 'target': torch.FloatTensor(target)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPpIPlkr8vsx"
   },
   "source": [
    "## Parte 2: Esqueleto de la red neuronal\n",
    "\n",
    "Definimos el modelo a entrenar durante la búsqueda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LVWbyNkq8vsz"
   },
   "outputs": [],
   "source": [
    "class IMDB_LSTM(nn.Module):\n",
    "    def __init__(self,\n",
    "                 pretrained_embeddings_path,\n",
    "                 dictionary,\n",
    "                 embedding_size,\n",
    "                 hidden_layer=32,\n",
    "                 num_layers=1,\n",
    "                 dropout=0.,\n",
    "                 bias=True,\n",
    "                 bidirectional=False,\n",
    "                 freeze_embedings=True):\n",
    "\n",
    "        super(IMDB_LSTM, self).__init__()\n",
    "\n",
    "        output_size = 1\n",
    "\n",
    "        # Create the Embeddings layer and add pre-trained weights\n",
    "        embeddings_matrix = torch.randn(len(dictionary), embedding_size)\n",
    "        embeddings_matrix[0] = torch.zeros(embedding_size)\n",
    "        with gzip.open(pretrained_embeddings_path, 'rt') as fh:\n",
    "            for line in fh:\n",
    "                word, vector = line.strip().split(None, 1)\n",
    "                if word in dictionary.token2id:\n",
    "                    wordID = dictionary.token2id[word]\n",
    "                    embeddings_matrix[wordID] = torch.FloatTensor([float(n) for n in vector.split()])\n",
    "\n",
    "        self.embedding_config = {'freeze': freeze_embedings, 'padding_idx': 0}\n",
    "        self.embeddings = nn.Embedding.from_pretrained(embeddings_matrix, **self.embedding_config)\n",
    "\n",
    "        # Set our LSTM parameters\n",
    "        self.lstm_config = {'input_size': embedding_size,\n",
    "                            'hidden_size': hidden_layer,\n",
    "                            'num_layers': num_layers,\n",
    "                            'bias': bias,\n",
    "                            'batch_first': True,\n",
    "                            'dropout': dropout if num_layers > 1 else 0.0,\n",
    "                            'bidirectional': bidirectional}\n",
    "\n",
    "        # Set our Fully Connected layer parameters\n",
    "        self.linear_config = {'in_features': hidden_layer,\n",
    "                              'out_features': output_size,\n",
    "                              'bias': bias}\n",
    "\n",
    "        # Instanciate the layers\n",
    "        self.lstm = nn.LSTM(**self.lstm_config)\n",
    "        self.droupout_layer = nn.Dropout(dropout)\n",
    "        self.classification_layer = nn.Linear(**self.linear_config)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedding = self.embeddings(inputs)\n",
    "        lstm_out, _ = self.lstm(embedding)\n",
    "        # Take last state of lstm, which is a representation of the entire text\n",
    "        lstm_out = lstm_out[:, -1, :].squeeze()\n",
    "        lstm_out = self.droupout_layer(lstm_out)\n",
    "        predictions = self.activation(self.classification_layer(lstm_out))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ej4JfQx57nmw"
   },
   "source": [
    "Encapsularemos el algoritmo de entrenamiento dentro de una función parametrizable. La función debería devolver los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "JO279TL-2BMr"
   },
   "outputs": [],
   "source": [
    "# Some default values...\n",
    "EPOCHS = 3\n",
    "MAX_SEQUENCE_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LHnG8wj07mX-"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_IMDB_model(train_dataset,\n",
    "                     val_dataset,\n",
    "                     pretrained_embeddings_path,\n",
    "                     dictionary,\n",
    "                     embedding_size,\n",
    "                     batch_size=128,\n",
    "                     max_sequence_len=MAX_SEQUENCE_LEN,\n",
    "                     hidden_layer=32,\n",
    "                     dropout=0.,\n",
    "                     epochs=EPOCHS,\n",
    "                     lr=0.001,\n",
    "                     optimizer_class=optim.Adam,\n",
    "                     verbose=False):\n",
    "    # Some weird way to control printing...\n",
    "    if verbose:\n",
    "        print_fn = print\n",
    "    else:\n",
    "        print_fn = lambda *x: None\n",
    "\n",
    "    # We define again the data loaders since this code could run in parallel\n",
    "    pad_sequeces = PadSequences(max_length=max_sequence_len)\n",
    "    # Train\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=pad_sequeces,\n",
    "                              drop_last=False)\n",
    "    # Val\n",
    "    val_loader = DataLoader(val_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            collate_fn=pad_sequeces,\n",
    "                            drop_last=False)\n",
    "\n",
    "    # Check if we have a GPU available\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device('cuda') if use_cuda else torch.device('cpu')\n",
    "    # We are not going to explore all hyperparameters, only these ones\n",
    "    model = IMDB_LSTM(pretrained_embeddings_path,\n",
    "                      dictionary,\n",
    "                      embedding_size,\n",
    "                      hidden_layer=hidden_layer,\n",
    "                      dropout=dropout)\n",
    "    # Don't forget to send the model to GPU if there is one available!\n",
    "    model.to(device)\n",
    "\n",
    "    loss_function = nn.BCELoss()\n",
    "    optimizer = optimizer_class(model.parameters(), lr)\n",
    "\n",
    "    history = {'train_loss': [], 'test_loss': [], 'test_avp': []}\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = []\n",
    "        print_fn(f'Epoch: {epoch}')\n",
    "        for idx, batch in enumerate(train_loader):\n",
    "            # We need to send everything to the device!\n",
    "            data = batch['data'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "            # Continue the training...\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss_value = loss_function(output.squeeze(), target)\n",
    "            loss_value.backward()\n",
    "            optimizer.step()\n",
    "            running_loss.append(loss_value.item())\n",
    "        train_loss = sum(running_loss) / len(running_loss)\n",
    "        print_fn(f'\\t Final train_loss: {train_loss}')\n",
    "        history['train_loss'].append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        running_loss = []\n",
    "        targets = []\n",
    "        predictions = []\n",
    "        for batch in val_loader:\n",
    "            # We need to send everything to the device!\n",
    "            data = batch['data'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "            # Continue the evaluation...\n",
    "            output = model(data)\n",
    "            loss_value = loss_function(output.squeeze(), target)\n",
    "            running_loss.append(loss_value.item())\n",
    "            targets.extend(target.cpu().numpy())\n",
    "            # Round up model output to get the predictions\n",
    "            predictions.extend(output.cpu().squeeze().round().detach().numpy())\n",
    "        test_loss = sum(running_loss) / len(running_loss)\n",
    "        avp = metrics.average_precision_score(targets, predictions)\n",
    "        print_fn(f'\\t Final test_loss: {test_loss}')\n",
    "        print_fn(f'\\t Final test_avp: {avp}')\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_avp'].append(avp)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WEzziR8j8vs0",
    "outputId": "d0f4e0b4-8646-4234-d4b3-19492be26298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "\t Final train_loss: 0.6834384832382202\n",
      "\t Final test_loss: 0.6922965021360488\n",
      "\t Final test_avp: 0.5091856556981886\n",
      "Epoch: 1\n",
      "\t Final train_loss: 0.690452372789383\n",
      "\t Final test_loss: 0.6872546313300966\n",
      "\t Final test_avp: 0.5203643509833626\n",
      "Epoch: 2\n",
      "\t Final train_loss: 0.6882177255153656\n",
      "\t Final test_loss: 0.6921687164003887\n",
      "\t Final test_avp: 0.5065320347062992\n"
     ]
    }
   ],
   "source": [
    "history = train_IMDB_model(train_dataset,\n",
    "                           val_dataset,\n",
    "                           pretrained_embeddings_path='data/glove.6B.50d.txt.gz',\n",
    "                           dictionary=preprocess.dictionary,\n",
    "                           embedding_size=50,\n",
    "                           verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDw_sn_E8vs1"
   },
   "source": [
    "## Utilizando `hyperopt`\n",
    "\n",
    "Para utilizar alguno de los algoritmos de *hyperopt*, es necesario definir una función objetivo que será minimizada. Esta función recibe un objeto con los valores para los hiperparámetros de cada experimento, y debe devolver una única métrica (o un diccionario con la clave `key` asociada a dicha métrica). En nuestro caso, utilizaremos el *average precision score* obtenido en el conjunto de validación.\n",
    "\n",
    "Les recomendamos consultar el [Tutorial](https://github.com/hyperopt/hyperopt/wiki/FMin) para más detalles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "G6-fXTPFDcLZ"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hyperopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2421911/740288508.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhyperopt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSTATUS_OK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Define an objective function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mobjective_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Exploring config: {args}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hyperopt'"
     ]
    }
   ],
   "source": [
    "from hyperopt import STATUS_OK\n",
    "\n",
    "# Define an objective function\n",
    "def objective_fn(args):\n",
    "    print(f'Exploring config: {args}')\n",
    "    # The references to train_dataset and val_dataset are taken from the global context!\n",
    "    history = train_IMDB_model(\n",
    "        train_dataset,\n",
    "        val_dataset,\n",
    "        pretrained_embeddings_path='data/glove.6B.50d.txt.gz',\n",
    "        dictionary=preprocess.dictionary,\n",
    "        embedding_size=50,\n",
    "        **args)\n",
    "\n",
    "    # This is the value that will be minimized!\n",
    "    history['loss'] = history['test_avp'][-1] * -1\n",
    "    # This is a required key\n",
    "    history['status'] = STATUS_OK\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2pgRRblWEYfQ",
    "outputId": "fc89f7e0-1f84-47bd-c436-b77cb1a64d36"
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "\n",
    "# Define the search space...\n",
    "space = {\n",
    "    'lr': hp.loguniform('lr', numpy.log(0.0001), numpy.log(0.005)), # See appendix\n",
    "    'optimizer_class': hp.choice('optimizer_class', [optim.Adam, optim.RMSprop]),\n",
    "    'dropout': hp.uniform('dropout', 0, 0.5)\n",
    "}\n",
    "\n",
    "# Define the Trials object, which will allow us to store information from every experiment.\n",
    "trials = Trials()\n",
    "# Minimize the objective function over the space\n",
    "best = fmin(objective_fn, space, algo=tpe.suggest, max_evals=10, trials=trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9LqRPCMxlZGy",
    "outputId": "80b80d84-5e13-4421-8764-fd71f84f5180"
   },
   "outputs": [],
   "source": [
    "print('Best hyperparameters...')\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mnMwhPRbuGOJ",
    "outputId": "284cbf8f-d3ca-406f-b7c1-8de55a9add38"
   },
   "outputs": [],
   "source": [
    "# We can see the results of each experiment with the trials object.\n",
    "trials.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "5qRtbTtz8vs4"
   },
   "source": [
    "## Recomendaciones finales\n",
    "\n",
    "* Es recomendable utilizar un parámetro de *paciencia*, que corta el ciclo de entrenamiento cuando no detecta mejoras en el desempeño sobre el conjunto de validación por **n** cantidad de épocas. Esto ayudaría a evitar que el modelo sobreajuste.\n",
    "* Realizar una búsqueda de grilla previa para determinar los valores para el optimizador, tasa de aprendizaje, tamaño de lote y número de épocas mínimas de entrenamiento, ya que estos son hiperparámetros muy determinantes.\n",
    "* No es necesario realizar la búsqueda de hiperparámetros sobre el conjunto de datos entero, ni entrenar el clasificador durante todas las épocas hasta que comienza a diverger. Se puede utilizar para encontrar los espacios más prometedores de valores posibles, y luego realizar una segunda búsqueda con menos iteraciones pero con el proceso de entrenamiento completo.\n",
    "* No realizar la búsqueda utilizando *notebooks*, sino **scripts**.\n",
    "* Combinar `hyperopt` con `mlflow` para un registro de los resultados ordenado.\n",
    "* Modificar el bucle de entrenamiento para guardar el último modelo con las mejores métricas en el conjunto de validación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmioKhtgpJ5Q"
   },
   "source": [
    "## Apéndice: hp.loguniform\n",
    "\n",
    "Según la documentación oficial, la distribución `hp.loguniform`:\n",
    "* Returns a value drawn according to `exp(uniform(low, high))` so that the logarithm of the return value is uniformly distributed.\n",
    "* When optimizing, this variable is constrained to the interval [exp(low), exp(high)].\n",
    "\n",
    "Supongamos que queremos que nuestros valores de **lr** se distribuyan logarítmicamente en el intervalo [0.0001, 0.005], entonces los valores de *low* y *high* deberían ser: `log(0.0001)` y  `log(0.005)`. Veamos qué distribución de muestras obtenemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "id": "aNUSEDBnpgDu",
    "outputId": "a57dd260-824f-4b76-8972-d44fe9cbbeba"
   },
   "outputs": [],
   "source": [
    "low = np.log(0.0001)\n",
    "high = np.log(0.005)\n",
    "\n",
    "sample_size = 1000\n",
    "\n",
    "sample = np.exp(np.random.uniform(low, high, size=sample_size))\n",
    "\n",
    "seaborn.displot(sample)\n",
    "seaborn.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QRiAOsncqBGz",
    "outputId": "81701624-3642-4b80-b141-e25ab6073a82"
   },
   "outputs": [],
   "source": [
    "sample.max(), sample.min()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "8_automated_hyperparameter_search.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
